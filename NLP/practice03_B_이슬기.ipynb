{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from builtins import ascii, bytes, chr, dict, filter, hex, input, int, map, next, oct, open, pow, range, round, str, \\\n",
    "  super, zip\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import collections\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SESSION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/pirl/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq examples..\n",
      "['dimly', ',', 'he', 'heard', 'laughter', ',', 'hoots', 'of', 'derision', ',', 'but', 'he', 'could', 'not', 'read', 'the', 'racket', 'properly', '.']\n",
      "['the', 'state', 'will', 'appeal', '--', 'but', 'few', 'think', 'it', 'will', 'actually', 'try', 'to', 'close', 'the', 'university', '.']\n",
      "['i', 'know', 'now', 'why', 'the', 'students', 'insisted', 'that', 'i', 'go', 'to', 'hiroshima', 'even', 'when', 'i', 'told', 'them', 'i', \"didn't\", 'want', 'to', '.']\n"
     ]
    }
   ],
   "source": [
    "# 변수들 설정\n",
    "rand = random.Random(0)\n",
    "embed_size = 256 # 워드 임베딩 차원 크기\n",
    "state_size = 256 # LSTM 히든 차원 크기\n",
    "max_epochs = 100 # 최대 학습 횟수\n",
    "minibatch_size = 20 # Minibatch 크기\n",
    "min_token_freq = 3 # 단어 최소 등장 빈도\n",
    "\n",
    "run_start = timeit.default_timer() # 시간 측정\n",
    "\n",
    "print('Loading raw data...')\n",
    "nltk.download('brown') # nltk에서 brown 말뭉치 다운로드\n",
    "\n",
    "# all_seqs = [['and', 'how', 'right', 'she', 'was', '.'], ['the', 'operator', 'asked', 'pityingly', '.']]\n",
    "# brown 말뭉치 문장(길이가 x 이상 y이하) 추출 -> 추출된 문장의 단어들을 소문자화 후 list에 보관\n",
    "# all_seqs = [['i', 'am', 'a', 'boy],['you', 'are', 'a', 'girl']...]\n",
    "all_seqs = [[token.lower() for token in seq] for seq in nltk.corpus.brown.sents() if 5 <= len(seq) <= 30]\n",
    "rand.shuffle(all_seqs) # all_seqs 랜덤 섞기\n",
    "all_seqs = all_seqs[:50000] # all_seqs 중에 20000개만 사용\n",
    "\n",
    "# print\n",
    "print('seq examples..')\n",
    "for seq in all_seqs[:3]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 39105\n",
      "Validation set size: 4345\n"
     ]
    }
   ],
   "source": [
    "trainingset_size = round(0.9 * len(all_seqs)) # 9할을 학습데이터로\n",
    "train_seqs = all_seqs[:trainingset_size] # 처음부터 trainingset_size까지를 train 데이터로\n",
    "validationset_size = len(all_seqs) - trainingset_size # 1할을 검증데이터로\n",
    "val_seqs = all_seqs[-validationset_size:] # 처음부터 validationset_size까지를 valid 데이터로\n",
    "\n",
    "print('Training set size:', trainingset_size)\n",
    "print('Validation set size:', validationset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14326\n",
      "token_to_index:\n",
      "<pad>: 0\n",
      "<edge>: 1\n",
      "<unk>: 2\n",
      "the: 3\n",
      ".: 4\n",
      ",: 5\n",
      "of: 6\n",
      "and: 7\n",
      "to: 8\n",
      "a: 9\n",
      "\n",
      "index_to_token:\n",
      "0: <pad>\n",
      "1: <edge>\n",
      "2: <unk>\n",
      "3: the\n",
      "4: .\n",
      "5: ,\n",
      "6: of\n",
      "7: and\n",
      "8: to\n",
      "9: a\n"
     ]
    }
   ],
   "source": [
    "all_tokens = (token for seq in train_seqs for token in seq) # all_tokens = ('and', 'how', 'right', ... )\n",
    "token_freqs = collections.Counter(all_tokens) # {'boy': freq, 'girl':freq, ....}\n",
    "vocab = sorted(token_freqs.keys(), key=lambda token: (-token_freqs[token], token)) # 단어 빈도수 정렬\n",
    "# 단어 최소 등장 빈도 이하인 단어들 모두 제거\n",
    "while token_freqs[vocab[-1]] < min_token_freq:\n",
    "  vocab.pop()\n",
    "vocab_size = len(vocab) + 3  # 문장 맨 앞, 맨 뒤 마커로 +1, 미등록어로 +1, PADDING +1\n",
    "\n",
    "edge_index = 1 # 문장 맨 앞, 맨 뒤 위치\n",
    "unknown_index = 2 # 미등록어 위치\n",
    "\n",
    "token_to_index = {'<pad>':0, '<edge>':1, '<unk>':2}\n",
    "token_to_index.update({token: i + 3 for (i, token) in enumerate(vocab)}) # 단어에 고유 번호 부여\n",
    "index_to_token = {v:k for k,v in token_to_index.items()} # 고유 번호에 단어 부여\n",
    "\n",
    "# print..\n",
    "print('Vocabulary size:', vocab_size)\n",
    "print('token_to_index:')\n",
    "for tok, idx in token_to_index.items():\n",
    "  if idx < 10: print(\"%s: %d\" % (tok,idx))\n",
    "\n",
    "print('\\nindex_to_token:')\n",
    "for idx, tok in index_to_token.items():\n",
    "  if idx < 10: print(\"%d: %s\" % (idx,tok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set max length: 30\n",
      "Validation set max length: 30\n",
      "train_seqs_in:\n",
      " [[    1  6804     5    12   346  4612     5     2     6 12187     5    32\n",
      "     12    71    31   558     3 10914  1982     4     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [    1     3   138    56  1662    49    32   172   181    16    56   621\n",
      "    603     8   406     3   550     4     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [    1    26   127    85   192     3   459  2520    17    26   135     8\n",
      "   6262    98    58    26   198    66    26   187   223     8     4     0\n",
      "      0     0     0     0     0     0     0]]\n",
      "train_seqs_out:\n",
      " [[ 6804     5    12   346  4612     5     2     6 12187     5    32    12\n",
      "     71    31   558     3 10914  1982     4     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [    3   138    56  1662    49    32   172   181    16    56   621   603\n",
      "      8   406     3   550     4     1     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [   26   127    85   192     3   459  2520    17    26   135     8  6262\n",
      "     98    58    26   198    66    26   187   223     8     4     1     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# train, valid input을 생성하는 함수\n",
    "def parse(seqs):\n",
    "  indexes = list()\n",
    "  lens = list()\n",
    "  for seq in seqs:\n",
    "    # 문장 속 단어들을 모두 고유 번호로 변환\n",
    "    indexes_ = [token_to_index.get(token, unknown_index) for token in seq]\n",
    "    indexes.append(indexes_) # 고유 번호로 표현된 문장들 list에 추가\n",
    "    lens.append(len(indexes_) + 1)  # 모든 문장들의 길이 정보 저장 +1은 문장 맨 앞 혹은 맨 뒤 마커 추가 때문\n",
    "\n",
    "  maxlen = max(lens) # 가장 긴 문장의 길이 정보\n",
    "\n",
    "  in_mat = np.zeros((len(indexes), maxlen), dtype=np.int) # LSTM 입력 문장 형태\n",
    "  out_mat = np.zeros((len(indexes), maxlen), dtype=np.int) # LSTM 출력 문장 형태\n",
    "  for (row, indexes_) in enumerate(indexes):\n",
    "    in_mat[row, :len(indexes_) + 1] = [edge_index] + indexes_ # 문장의 시작에 edge 마커 추가\n",
    "    out_mat[row, :len(indexes_) + 1] = indexes_ + [edge_index] # 문장의 끝에 edge 마커 추가\n",
    "  return (in_mat, out_mat, np.array(lens))\n",
    "\n",
    "# train_seqs_in = [[1,423,23,8,656],[1,23,3,2,86]...]\n",
    "# train_seqs_out = [[423,23,8,656,1],[23,3,2,86,1]...]\n",
    "# train_seqs_len = [4,2,3,5, ...]\n",
    "(train_seqs_in, train_seqs_out, train_seqs_len) = parse(train_seqs)\n",
    "(val_seqs_in, val_seqs_out, val_seqs_len) = parse(val_seqs)\n",
    "\n",
    "print('Training set max length:', train_seqs_in.shape[1] - 1)\n",
    "print('Validation set max length:', val_seqs_in.shape[1] - 1)\n",
    "print('train_seqs_in:\\n', train_seqs_in[:3])\n",
    "print('train_seqs_out:\\n', train_seqs_out[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "WARNING:tensorflow:From <ipython-input-7-08b5fec112f4>:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "# Build Graph #\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "# Full correct sequence of token indexes with start token but without end token.\n",
    "# LSTM의 입력\n",
    "seq_in = tf.placeholder(tf.int32, shape=[None, None], name='seq_in')  # [seq, token]\n",
    "\n",
    "# Length of sequences in seq_in.\n",
    "seq_len = tf.placeholder(tf.int32, shape=[None], name='seq_len')  # [seq]\n",
    "\n",
    "# Full correct sequence of token indexes without start token but with end token.\n",
    "# LSTM의 출력 즉 정답\n",
    "seq_target = tf.placeholder(tf.int32, shape=[None, None], name='seq_target')  # [seq, token]\n",
    "batch_size = tf.shape(seq_in)[0]  # Number of sequences to process at once.\n",
    "num_steps = tf.shape(seq_in)[1]  # Number of tokens in generated sequence. # LSTM에서 순환의 횟수\n",
    "\n",
    "# Mask of which positions in the matrix of sequences are actual labels as opposed to padding.\n",
    "# 나중 학습 에러를 계산하기 위한 마스킹\n",
    "# tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
    "                                  #  [True, True, True, False, False],\n",
    "                                  #  [True, True, False, False, False]]\n",
    "token_mask = tf.cast(tf.sequence_mask(seq_len, num_steps), tf.float32)  # [seq, token]\n",
    "\n",
    "with tf.variable_scope('prefix_encoder'):\n",
    "  # Encode each sequence prefix into a vector.\n",
    "\n",
    "  # Embedding matrix for token vocabulary.\n",
    "  # 임베딩 변수 선언 초기화 값은 xavier initializer 사용\n",
    "  embeddings = tf.get_variable('embeddings', [vocab_size, embed_size], tf.float32,\n",
    "                               tf.contrib.layers.xavier_initializer())  # [vocabulary token, token feature]\n",
    "\n",
    "  # 3D tensor of tokens in sequences replaced with their corresponding embedding vector.\n",
    "  # seq_in [seq, token] -> [seq, token, emb_feature]로 lookup\n",
    "  embedded = tf.nn.embedding_lookup(embeddings, seq_in)  # [seq, token, emb_feature]\n",
    "\n",
    "  # Use an LSTM to encode the generated prefix.\n",
    "  # LSTM의 초기 state 선언 LSTM initial state = (c_vec,h_vec) 튜플로 구성됨\n",
    "  init_state = tf.contrib.rnn.LSTMStateTuple(c=tf.zeros([batch_size, state_size]), h=tf.zeros([batch_size, state_size]))\n",
    "  cell = tf.contrib.rnn.BasicLSTMCell(state_size) # 순환 신경망의 Cell을 정의\n",
    "  # 실제 LSTM cell을 가지는 RNN 네트워크를 생성 return 값은 (output, state) 튜플\n",
    "  prefix_vectors = tf.nn.dynamic_rnn(\n",
    "    cell, embedded, sequence_length=seq_len, \n",
    "    initial_state=init_state, scope='rnn')[0]  # [seq, prefix, prefix feature] # (outputs, final_state[batch_size, cell.state_size]) tuple\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "  # Output a probability distribution over the token vocabulary (including the end token).\n",
    "  # 최종 생성 단어 확률 분포를 구하는 부분 (state, state_size)*(state_size, vocab_size) + b = (state, vocab_size)\n",
    "  W = tf.get_variable('W', [state_size, vocab_size], tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "  b = tf.get_variable('b', [vocab_size], tf.float32, tf.zeros_initializer())\n",
    "  logits = tf.reshape(tf.matmul(tf.reshape(prefix_vectors, [-1, state_size]), W) + b,\n",
    "                      [batch_size, num_steps, vocab_size])\n",
    "  predictions = tf.nn.softmax(logits)  # [seq, prefix, token]\n",
    "\n",
    "# 내부적으로 softmax를 취한 뒤 loss를 계산 한다. 따라서 입력이 logits이다. * token_mask로 해당사항 없는 것들은 0으로 만듬\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=seq_target, logits=logits) * token_mask\n",
    "total_loss = tf.reduce_sum(losses) # 문장 내 모든 단어에서 발생하는 loss의 합\n",
    "train_step = tf.train.AdamOptimizer().minimize(total_loss)\n",
    "saver = tf.train.Saver() # 변수들을 저장하고 불러오기 위한 클래스 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_SESSION:\n",
    "  # () 속의 텐서들을 계산하거나 또는 () 속의 오퍼레이션을 실행\n",
    "  # 따라서 그래프 내의 변수들을 초기화하는 오퍼레이션을 실행\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  print('epoch', 'val loss', 'duration', sep='\\t')\n",
    "\n",
    "  epoch_start = timeit.default_timer()\n",
    "\n",
    "  validation_loss = 0\n",
    "  for i in range(len(val_seqs) // minibatch_size):\n",
    "    # seq_in 0부터 minibatch_size 까지를 입력으로 i번째 minibatch를 수행\n",
    "    # feed_dict는 그래프의 입력값, total_loss는 최종 그래프에서 구하게 될 텐서\n",
    "    minibatch_validation_loss = sess.run(total_loss, feed_dict={\n",
    "      seq_in: val_seqs_in[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "      seq_len: val_seqs_len[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "      seq_target: val_seqs_out[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "    })\n",
    "    # 모든 minibatch들의 loss를 더해주는 것\n",
    "    validation_loss += minibatch_validation_loss\n",
    "\n",
    "  print(0, round(validation_loss, 3), round(timeit.default_timer() - epoch_start), sep='\\t')\n",
    "  last_validation_loss = validation_loss\n",
    "  \n",
    "  # 최초로 초기화 된 변수들을 모델로 저장함\n",
    "  saver.save(sess, './model')\n",
    "\n",
    "  #trainingset_indexes = [0,1,2,3,4,...len(train_seqs)]\n",
    "  trainingset_indexes = list(range(len(train_seqs)))\n",
    "  for epoch in range(1, max_epochs + 1):\n",
    "    epoch_start = timeit.default_timer()\n",
    "\n",
    "    rand.shuffle(trainingset_indexes)\n",
    "    for i in range(len(trainingset_indexes) // minibatch_size):\n",
    "      # validation하고 다르게 random suffle이 들어가서 아래 내용이 조금 달라진 것\n",
    "      minibatch_indexes = trainingset_indexes[i * minibatch_size:(i + 1) * minibatch_size]\n",
    "      a = sess.run([train_step, logits, seq_target], feed_dict={\n",
    "        seq_in: train_seqs_in[minibatch_indexes],\n",
    "        seq_len: train_seqs_len[minibatch_indexes],\n",
    "        seq_target: train_seqs_out[minibatch_indexes],\n",
    "      })\n",
    "\n",
    "    validation_loss = 0\n",
    "    for i in range(len(val_seqs) // minibatch_size):\n",
    "      minibatch_validation_loss = sess.run(total_loss, feed_dict={\n",
    "        seq_in: val_seqs_in[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        seq_len: val_seqs_len[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "        seq_target: val_seqs_out[i * minibatch_size:(i + 1) * minibatch_size],\n",
    "      })\n",
    "      validation_loss += minibatch_validation_loss\n",
    "      \n",
    "    # early_stop 조건\n",
    "    if validation_loss > last_validation_loss:\n",
    "      break\n",
    "    last_validation_loss = validation_loss\n",
    "\n",
    "    saver.save(sess, './model')\n",
    "\n",
    "    print(epoch, round(validation_loss, 3), round(timeit.default_timer() - epoch_start), sep='\\t')\n",
    "\n",
    "  print(epoch, round(validation_loss, 3), round(timeit.default_timer() - epoch_start), sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "P(the dog barked.) = 8.345212e-09\n",
      "P(the cat barked.) = 1.6606981e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Evaluating...')\n",
    "\n",
    "# 저장된 모델 불러오는 것\n",
    "saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "### 실습 함수\n",
    "def seq_prob(seq):\n",
    " seq_indexes = [token_to_index.get(token, unknown_index) for token in seq ] # token_to_index.get(token, unknown_index) 를 이용해 완성할 것 (1)\n",
    " outputs = sess.run(predictions, feed_dict={\n",
    "                                     seq_in:  [ [ edge_index ] + seq_indexes ],\n",
    "                                     seq_len: [ 1+len(seq) ],\n",
    "                                   })[0] # feed_dict={} 를 완성할 것 (2)\n",
    " probs = outputs[np.arange(len(outputs)-1), seq_indexes]\n",
    " return np.prod(probs)\n",
    "\n",
    "print('P(the dog barked.) =', seq_prob(['the', 'dog', 'barked', '.']))\n",
    "print('P(the cat barked.) =', seq_prob(['the', 'cat', 'barked', '.']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
